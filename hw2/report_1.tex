\documentclass[11pt]{article}

\newcommand{\cnum}{CS188}
\newcommand{\ced}{Winter 2017}
\newcommand{\ctitle}[3]{\title{\vspace{-0.5in}\cnum, \ced\\Problem Set #1: #2\\Due #3}}
\usepackage{enumitem}
\newcommand{\solution}[1]{{{\color{blue}{\bf Solution:} {#1}}}}
\usepackage[usenames,dvipsnames,svgnames,table,hyperref]{xcolor}
\usepackage{graphicx} %package to manage images
\graphicspath{ {/} }

\renewcommand*{\theenumi}{\alph{enumi}}
\renewcommand*\labelenumi{(\theenumi)}
\renewcommand*{\theenumii}{\roman{enumii}}
\renewcommand*\labelenumii{\theenumii.}


\begin{document}
\ctitle{2}{}{2/9/2017}
\author{Shannon Phu}
\date{}
\maketitle
\vspace{-0.3in}

\section{Problem 1}
    \begin{enumerate}
        \item
            w1 = 1,
            w2 = 1,
            b = -1
        \item 
            XOR's feature values are not linearly separable so it cannot be represented by a perceptron.
        
        \vfill
        \clearpage
    
    \end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Problem 2}
    \begin{enumerate}
        \item 
            \begin{equation}
                h_{\theta} = \frac{1}{1+e^{-\theta x_{n}}}
            \end{equation}
            \begin{equation}
                log(h_{\theta}(x_{n})) = -log(1+e^{-\theta x_{n}})
            \end{equation}
            \begin{equation}
                log(1-h_{\theta}(x_{n})) = -\theta x_{n}-log(1+e^{-\theta x_{n}})
            \end{equation}
            \begin{equation}
                J(\theta) = \sum_{n=1}^{N} y_{n} log(1+e^{-\theta x_{n}}) + (1-y_{n})(\theta x_{n}+log(1+e^{-\theta x_n}))
            \end{equation}
            \begin{equation}
                = \sum_{n=1}^{N} -y_{n}\theta x_{n} + \theta x_{n} + log(1+e^{-\theta x_{n}})
            \end{equation}
            \begin{equation}
                = \sum_{n=1}^{N} -y_{n}\theta x_{n} + log(1+e^{\theta x_{n}})
            \end{equation}
            \begin{equation}
                \frac{\partial J(\theta)}{\partial \theta} = 
                    \sum_{n=1}^{N} -x_{n}y_{n} + \frac{x_{n}e^{\theta x_{n}}}{e^{\theta x_{n}}+1}
            \end{equation}
            \begin{equation}
                \frac{\partial J(\theta)}{\partial \theta} = 
                    \sum_{n=1}^{N} -x_{n}y_{n} + x_{n}h_{\theta}(x_{n})
            \end{equation}
            
            
        \item
            \begin{equation}
                \frac{\partial ^{2}J(\theta)}{\partial \theta _{j}\theta _{k}} = 
                    \sum_{n=1}^{N} x_{n_{j}}h_{\theta}'(x_{n})
            \end{equation}
            because
            \begin{equation}
                h_{\theta}'(x_{n}) = h_{\theta}(x_{n})(1-h_{\theta}(x_{n})) ,
            \end{equation}
            \begin{equation}
                \frac{\partial ^{2}J(\theta)}{\partial \theta _{j}\theta _{k}} = 
                    \sum_{n=1}^{N} x_{n_{j}}h_{\theta}(x_{n})(1-h_{\theta}(x_{n}))x_{n}^{T}
            \end{equation}
        
        
        \item 
            Because H is written as
            \begin{equation}
                H = \sum_{n=1}^{N} x_{n_{j}}h_{\theta}(x_{n})(1-h_{\theta}(x_{n}))x_{n}^{T} ,
            \end{equation}
            and we know that the function h, representing the sigmoid function, must be between 0 and 1 hence making 
            \begin{equation}
                h_{\theta}(x_{n})(1-h_{\theta}(x_{n}))
            \end{equation}
            positive.
            For J to be convex, H must be positive semi-definite. Therefore we can exclude Equation 13 from Equation 12, only paying attention to x_{n}.
            
            \begin{equation}
                z^{T}x_{n}x_{n}^{T}z = (x_{n}^{T}z)^{T}(x_{n}^{T}z) = (x_{n}^{T}z) \cdot (x_{n}^{T}z)
            \end{equation}
            
            The dot product is positive which means that the Hessian matrix is positive semi-definite thus J is convex.
        
    \end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Problem 3}
    \begin{enumerate}
        \item 
            \begin{equation}
                \frac{\partial J(\theta)}{\partial \theta _{0}} = 
                    2\sum_{n=1}^{N} w_{n}(\theta _{0} + \theta _{1}x_{n,1}-y_{n})
            \end{equation}
            \begin{equation}
                \frac{\partial J(\theta)}{\partial \theta _{1}} = 
                    2\sum_{n=1}^{N} w_{n}(\theta _{0} + \theta _{1}x_{n,1}-y_{n})x_{n,1}
            \end{equation}
        
        \item
            A system of equations was made from part (a) as follows:
            \begin{equation}
                \sum_{n=1}^{N} w_{n}y_{n} = \theta _{0} \sum_{n=1}^{N}w_{n} + \theta _{1}\sum_{n=1}^{N} w_{n}x_{n,1}
            \end{equation}
            \begin{equation}
                \sum_{n=1}^{N} w_{n}x_{n,1}y_{n} = \theta _{0} \sum_{n=1}^{N}w_{n}x_{n,1} + \theta _{1}\sum_{n=1}^{N} w_{n}x_{n,1}^{2}
            \end{equation}
            
            Solving for theta 0 and 1:
            \begin{equation}
                \theta _{0} = \frac{\sum_{n=1}^{N}w_{n}y_{n} - \theta _{1}\sum_{n=1}^{N}w_{n}x_{n,1}}{\sum_{n=1}^{N}w_{n}}
            \end{equation}
            
            Plugging it into Equation 18, I can solve for theta 1:
            \begin{equation}
                \theta _{1} = \frac{\sum_{n=1}^{N}w_{n} \sum_{n=1}^{N}w_{n}x_{n,1}y_{n} - \sum_{n=1}^{N}w_{n}y_{n}\sum_{n=1}^{N}w_{n}x_{n}}{\sum_{n=1}^{N}w_{n} \sum_{n=1}^{N}w_{n}x_{n,1}^{2} - (\sum_{n=1}^{N}w_{n}x_{n,1})^{2}}
            \end{equation}
            
            Plugging Equation 20 back into Equation 19, we can solve for theta 0 as well:
            \begin{equation}
                \theta _{0} = 
                    \frac{\sum_{n=1}^{N}w_{n}y_{n}}{\sum_{n=1}^{N}w_{n}} -
                    \frac{\sum_{n=1}^{N}w_{n}x_{n,1}}{\sum_{n=1}^{N}w_{n}}
                    \cdot
                    \theta _{1}
            \end{equation}
        
    \end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
